# ==============================================================================
# SenTient NLP Configuration (Falcon Layer)
# ==============================================================================
# Role: Controls the Semantic Relation Extraction and Contextual Re-ranking.
# Status: PRODUCTION (Docker Optimized)
# ==============================================================================

server:
  host: "0.0.0.0"       # [DOCKER] Must bind to all interfaces to be reachable
  port: 5005
  workers: 1            # [CRITICAL] Keep at 1. Multi-worker crashes PyTorch on CPU.
  timeout: 300          # [PERFORMANCE] Increased to 5 mins for initial Model Load

elasticsearch:
  # [DOCKER NETWORKING] Must point to the container name, not localhost
  hosts: ["http://sentient_elastic:9200"]
  # Index names must match config/elastic/falcon_mapping.json
  indexes:
    properties: "sentient_properties_v1"
    entities: "sentient_entities_fallback" 
  connection:
    timeout: 30
    max_retries: 3
    retry_on_timeout: true

# ==============================================================================
# 1. PRE-PROCESSING STRATEGIES (The "Compression" Logic)
# ==============================================================================
preprocessing:
  language: "en"
  # Aggressive stopword removal to isolate predicates (e.g., "is buried in" -> "buried")
  stopwords_file: "data/stopwords/falcon_extended_en.txt"
  
  # Regex to clean surface forms before N-Gram generation
  clean_regex: "[^a-zA-Z0-9\\s]"
  
  # N-Gram Window for Relation Extraction (e.g., "Mayor of Paris" -> 3-gram)
  ngrams:
    min_length: 1
    max_length: 6 

# ==============================================================================
# 2. EMBEDDING MODEL (The "Brain")
# ==============================================================================
embeddings:
  # Using a lightweight SBERT model (80MB) for CPU speed/accuracy trade-off
  model_name: "all-MiniLM-L6-v2"
  cache_dir: "./models/cache"
  device: "cpu" # Set to "cuda" only if NVIDIA GPU is present
  normalize_embeddings: true 

# ==============================================================================
# 3. RE-RANKING STRATEGIES (The "Hybrid" Logic)
# ==============================================================================
ranking:
  # Strategy 1: Levenshtein Post-Processing
  # [ARCHITECTURAL DECISION] Disabled. String distance is handled by Layer 3 (Java Core).
  levenshtein:
    enabled: false
    max_distance_threshold: 2  
    exact_match_boost: 2.0      
    near_match_penalty: 0.5    

  # Strategy 2: Contextual Similarity
  # Measures distance between the Sentence Vector and Candidate Vector
  context:
    window_size: 5  # Words left/right of entity to capture
    weight: 0.7     # 70% Context, 30% Lexical match (implicitly)

# ==============================================================================
# 4. THRESHOLDS (The "Decision" Logic)
# ==============================================================================
thresholds:
  # Minimum score to even consider a candidate
  min_confidence: 0.35
  
  # Score above which the UI shows "MATCHED" automatically
  auto_match_cutoff: 0.85
  
  # [PERFORMANCE] Limit to 3 candidates per row to spare CPU cycles.
  # Calculating vectors for >3 candidates increases latency significantly.
  max_candidates: 3

# ==============================================================================
# 5. CACHING (The "Speed" Layer)
# ==============================================================================
cache:
  enabled: true
  backend: "redis" 
  # [DOCKER NETWORKING] Must point to the container name
  redis_url: "redis://sentient_redis:6379/0"
  ttl_seconds: 86400 # 24 Hours